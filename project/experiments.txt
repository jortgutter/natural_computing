BaseCNN:

Tried BatchNorm, but did not work at all :(

x # epochs (test with early stopping? Is a callback, later used for setting args.n_epochs)
x Change # layers
x Different # channels (default is powers of 2)

x Extra decision layer? (for non-linear combinations of features (no softmax activation for first))
   x More epochs for 2 layers

x Different activation functions for conv layers (relu, leaky relu, tanh)
x Different dropout probabilities (max 0.5)

Store # parameters, training-time and accuracies carefully

==================================================================================

No dropout!
Early stopping?

x 4 conv layers, no dropout
x 5 conv layers, fewer channels, no dropout
   - 6 layers?

x ADAM Optimizer

==================================================================================

Ensemble:

TODO:
- python3 main.py ensemble 10 5 4 ensemble_6ep_5conv_4chan_25nets.txt --n_nets 25

(- Test 3 classes as dropout)

- Error correlation
- Graph of ensemble size vs accuracy
- Graph of ensemble networks size (# layers/channels) vs accuracy
...
